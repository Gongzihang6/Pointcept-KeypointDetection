# OctFormer：Octree-based Transformers for 3D Point Clouds

## 创新与贡献

**核心思想与背景痛点**

这篇论文的核心在于提出了一种基于八叉树注意力机制（Octree Attention）的点云骨干网络，旨在解决传统点云 Transformer 的效率瓶颈。在以往的方法中，自注意力机制通常依赖于 KNN 或 Ball Query 来构建局部邻域。这意味着对于每一个中心点，网络都需要进行一次昂贵的最近邻搜索。这种方式不仅耗时，而且导致了大量的计算冗余——因为邻域之间往往高度重叠（点 A 既是点 B 的邻居，也可能是点 C 的邻居）。这种“滑动窗口”式的计算模式无法有效利用 GPU 的并行能力，成为了限制模型处理大规模点云的主要障碍。

**核心机制：从空间搜索到序列切分**

OctFormer 彻底摒弃了昂贵的邻域搜索，转而采用一种“排序+切分”的策略。具体而言，它首先对输入点云构建八叉树，利用非空叶子节点（体素）的整数坐标计算 Morton 码（Shuffled Key）。基于 Morton 码的排序将原本散乱的三维空间体素映射为一维的 Z-order 序列。Z-order 的特性保证了序列中相邻的元素在物理空间中也是紧密相邻的。

基于这个有序序列，OctFormer 直接按固定数量 $K$（例如 32 个点）进行截断划分，将序列切分为一个个互不重叠的局部窗口。虽然这种切分方式会导致窗口在三维空间中的几何形状变得不规则（不再是标准的立方体），但论文通过实验证明，自注意力机制对窗口的形状并不敏感，因此性能未受影响。这一设计的最大优势在于：它将复杂的空间邻域搜索简化为了极低成本的 Tensor Reshape 操作，计算复杂度从二次方降为线性，极大地提升了推理速度。

**感受野扩展与并行计算**

当然，这种硬性的序列切分也存在副作用：物理空间边界上的点可能会与其原本的几何邻居被划分到不同的窗口中，从而切断了局部的交互。为了缓解这一问题，论文引入了扩张注意力（Dilated Attention）。类似于空洞卷积，它通过在序列中间隔采样（例如每隔 4 个点选一个）来构建窗口，在不增加计算量的前提下显著扩大了感受野，促进了跨窗口的信息交流。这种固定点数的窗口划分策略（无论是标准还是扩张模式）都完美契合了 GPU 的 SIMD 架构，使得并行计算效率达到了极致。

**条件位置编码 (CPE)**

最后，针对窗口形状不规则的问题，传统的相对位置编码（Relative PE）难以直接应用。OctFormer 创新性地引入了条件位置编码（Conditional Positional Encoding, CPE）。它不再使用固定的编码表，而是利用八叉树特有的深度卷积（Depthwise Convolution），根据当前的特征状态动态生成位置编码。实验表明，这种动态编码方式比固定编码更适合处理非结构化的点云数据，进一步提升了模型性能。

## 网络架构

OctFormer的整体网络架构如下图所示：

![OctFormer的整体网络架构](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/2025%5C12%5Cimage-20251224150906853.png)

主要包括特征嵌入Embedding、OctFormer Block、Downsampling三个模块。

### Embedding

在进入神经网络之前，原始点云需要经过特定的结构化处理，这是 OctFormer 能高效运行的基石。假设原始点云包含M个点，每个点有坐标$(x,y,z)$和特征（如颜色RGB、法向量等）。

#### 数据预处理

第一步是对点云坐标进行归一化（论文中提到的是乘以一个缩放因子scale factor 0.01m），然后构建深度为11的八叉树，这相当于将空间划分为2048×2048×2048个微小体素，然后根据每个非空体素在这2048×2048×2048的立方体网格中的整数坐标，为每个非空体素计算Shuffle Key（Morton Code），并根据这个Key对非空体素进行排序（Z-order排序）。

然后是特征初始化，将属于同一个八叉树叶子节点的点的特征（位置、颜色、法向量）取平均值，作为这个非空体素节点的初始特征。因此初始张量形状为$(N_{raw},C_{in})$，其中$N_{raw}$是非空叶子节点的数量，$C_{in}$是输入特征维度（通常包含位置、颜色、法向量）。

#### 八叉树卷积

这一步的作用是将低维几何特征映射到高维特征空间，并进行初步的空间下采样。首先接收前一步排好序的非空体素的特征张量$(N_{raw},C_{in})$

计算过程：采用5层八叉树卷积序列（核心都是将输入特征和卷积核进行相乘并求和，然后滑动卷积窗口，并改变通道数、空间分辨率），采用的卷积核大小/步长序列为：(3,1) $\to$ (2,2) $\to$ (3,1) $\to$ (2,2) $\to$ (3,1) 。其中两次步长为2的卷积实现了空间分辨率的4倍下采样（相当于序列长度缩短为原来1/4）。

输出形状：空间维度：N=N_{raw}/4（近似），和图中标注的S/4一致。通道维度：从原始的位置+RGB\法向量映射到C（通常是96维）；处理后的张量形状为$(N,C)$。

### OctFormer Block

网络的核心结构，根据图示，两个连续的Block分别使用Dilation=1和Dilation=4。

![OctFormer Block](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/2025%5C12%5Cimage-20251224155934356.png)

#### 条件位置编码（CPE）

输入为前面5层八叉树卷积的结果$X=(N,C)$，使用八叉树深度卷积动态生成位置编码并加到输入上。具体计算方式如下：
$$
X=X+BatchNorm(DepthwiseConv(X))
$$
可能是因为X包含了八叉树卷积前的位置信息，再通过八叉树深度卷积，对位置信息进一步处理，得到能够表征位置的特征，加到原始的结果X上。

#### 标准八叉树注意力

##### Dilation=1

第一步是填充（Padding）：

我们需要检查N是否能被窗口大小K=32整除，如果不能整除，则补零到\hat{N}。

第二步是窗口划分（Window Partition）：

我们将输入张量(\hat{N},C)变为(B,K,C)，其中B=\hat{N}/32是窗口数量。此时，每个K维度内的32个点在Z序上是连续的（空间上也是聚集的）。

第三步是进行Attention的计算：

由于每个窗口内点的数量都一样，我们可以concat起来并行执行Multi-head Self-Attention，复杂度从O(N^2)降为近似线性O(KN)，K为窗口内点云个数

第四步是逆操作：

将attention计算结果Flatten回(\hat{N},C)，然后移除填充部分，恢复为(N,C)。

第五步是前馈网络MLP：

经过LayerNorm层归一化后进入MLP（两层全连接，中间GELU激活），通道膨胀比为4，通过第一层全连接层，通道数变为4C，第二层全连接层将通道数变回C。

##### Dilation=4

目的是扩大感受野，让信息在不同窗口间通过。具体原理类似扑克牌发牌，之前Dilation=1的时候，就像是给一个人连续发牌，发完32张作为一组，然后再接着给同一个人发，作为第二组，一个人会得到很多组牌，但是每组牌都是在Z序（物理空间）上相邻的，感受野有限；但是Dilation=4的时候，就相当于轮流给4个人发牌，先给第一个人发一张，再给第二个人发一张，再给第三个人发一张，最后给第四个人发一张，然后继续再给第一个人发一张，如此往复，直到每个人手里都有32张牌了，这就是一组，此时每个人手里的32张牌在Z序上不是相邻的，是隔了3个位置的，相当于原来只能看到32个点范围，现在能看到32×4=128个点范围，每个人32张牌是一组，然后接着发，为了保证每个人手里牌一样多，组数一致，因此必须要给序列填充补零，使得\hat{N}能被K\times D=32×4=128整除。具体实现步骤如下：

**扩张划分 (Dilated Partition)**:

-   **Reshape**: 变为 $(B', K, D, C)$，即 $(B', 32, 4, C)$。
-   **Transpose (转置)**: 交换维度变为 $(B', D, K, C) \to (B', 4, 32, C)$。
-   **Flatten**: 合并前两维 $\to (B' \times 4, 32, C)$ 13。

*解释*: 这相当于把原本连续的序列，按间隔 4 抽取点组成新窗口。

划分好窗口后，和之前一样，以组为基本单位concat起来并行执行attention，然后逆向执行扩张划分步骤，恢复(N,C)

### Downsampling

论文中使用的下采样模块比较简单，没有使用Max Pooling或Average Pooling这样的池化操作，而是直接使用步长为2的八叉树卷积来实现的，具体来说，使用的卷积核大小为2，步长为2（实现长度减半），但是通道数量加倍（卷积核数量设置为2C即可），卷积操作之后紧跟一个Batch Normalzation（批量归一化层）。

























































































